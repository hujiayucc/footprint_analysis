# src/model_training.py
import os
import sys
import joblib
import logging
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.model_selection import train_test_split

# ================== é…ç½®åŒº ==================
logging.basicConfig(
    level=logging.INFO,
    format="[%(asctime)s] [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("run.log", encoding="utf-8"),
        logging.StreamHandler()
    ]
)

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_PATH = os.path.join(BASE_DIR, "data", "raw", "footprint_data.csv")
MODEL_DIR = os.path.join(BASE_DIR, "models")
os.makedirs(MODEL_DIR, exist_ok=True)

# ================== æ ¸å¿ƒå‡½æ•°å®šä¹‰ ==================
def load_data():
    """åŠ è½½æ•°æ®é›†å¹¶éªŒè¯å®Œæ•´æ€§"""
    try:
        logging.info(f"ğŸ” æ­£åœ¨åŠ è½½æ•°æ®: {DATA_PATH}")
        df = pd.read_csv(DATA_PATH)
        
        required_columns = {
            "foot_length", "foot_width", "arch_height", 
            "pressure_avg", "ground_type", "humidity",
            "depth", "depth_diff", "pressure_offset_x",
            "height", "weight", "leg_type"
        }
        missing = required_columns - set(df.columns)
        if missing:
            raise ValueError(f"ç¼ºå¤±å…³é”®å­—æ®µ: {missing}")
            
        logging.info(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ | æ ·æœ¬æ•°: {len(df)}")
        return df
    except Exception as e:
        logging.error("â€¼ï¸ æ•°æ®åŠ è½½å¤±è´¥", exc_info=True)
        sys.exit(1)

def build_preprocessor():
    """æ„å»ºé¢„å¤„ç†ç®¡é“"""
    try:
        logging.info("ğŸ› ï¸ æ„å»ºé¢„å¤„ç†ç®¡é“...")
        return ColumnTransformer([
            ('cat', OneHotEncoder(), ['ground_type']),  # ç”Ÿæˆ5åˆ—
            ('num', StandardScaler(), [
                'foot_length', 'foot_width', 'arch_height',
                'pressure_avg', 'humidity', 'depth',
                'depth_diff', 'pressure_offset_x'
            ])  # 8ä¸ªæ•°å€¼ç‰¹å¾
        ])
    except Exception as e:
        logging.error("â€¼ï¸ é¢„å¤„ç†å™¨æ„å»ºå¤±è´¥", exc_info=True)
        sys.exit(1)

# ================== åŠ¨æ€æ¨¡å‹é…ç½® ==================
def calculate_model_capacity(data_size):
    """æ ¹æ®æ•°æ®é‡åŠ¨æ€è®¡ç®—æ¨¡å‹å‚æ•°"""
    base_units = 64
    scaling_factor = min(1.0, np.log10(data_size / 5000 + 1))  # è®¡ç®—ç¼©æ”¾å› å­
    
    return {
        "scaling_factor": scaling_factor,  # å¿…é¡»åŒ…å«æ­¤é¡¹
        "hidden_units": int(base_units * (1 + scaling_factor)),
        "num_layers": max(1, int(2 * scaling_factor)),
        "dropout_rate": max(0.1, 0.5 - 0.1 * scaling_factor)
    }

# ================== ä¼˜åŒ–åçš„æ¨¡å‹æ„å»º ==================
def build_dynamic_model(input_shape_cat, input_shape_num, model_cfg):
    """æ„å»ºåŠ¨æ€è°ƒæ•´çš„ç¥ç»ç½‘ç»œæ¨¡å‹"""
    try:
        logging.info("ğŸ§  æ„å»ºåŠ¨æ€æ¨¡å‹...")
        
        # è¾“å…¥å±‚
        input_cat = tf.keras.Input(shape=input_shape_cat, name="categorical_input")
        input_num = tf.keras.Input(shape=input_shape_num, name="numerical_input")
        
        # ç‰¹å¾èåˆ
        merged = tf.keras.layers.concatenate([input_cat, input_num])
        
        # åŠ¨æ€éšè—å±‚
        x = merged
        for _ in range(model_cfg["num_layers"]):
            x = tf.keras.layers.Dense(model_cfg["hidden_units"], activation="relu")(x)
            x = tf.keras.layers.BatchNormalization()(x)
            x = tf.keras.layers.Dropout(model_cfg["dropout_rate"])(x)
        
        # å¤šä»»åŠ¡è¾“å‡º
        reg_output = tf.keras.layers.Dense(2, name="regression")(x)
        cls_output = tf.keras.layers.Dense(3, activation="softmax", name="classification")(x)
        
        model = tf.keras.Model(
            inputs=[input_cat, input_num],
            outputs=[reg_output, cls_output]
        )
        
        # æ··åˆç²¾åº¦ä¼˜åŒ–
        tf.keras.mixed_precision.set_global_policy("mixed_float16")
        
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
            loss={
                "regression": "mse",
                "classification": "sparse_categorical_crossentropy"
            },
            metrics={
                "regression": ["mae"],
                "classification": ["accuracy"]
            }
        )
        logging.info(f"âœ… åŠ¨æ€æ¨¡å‹æ„å»ºæˆåŠŸ | éšè—å±‚: {model_cfg['num_layers']}x{model_cfg['hidden_units']}")
        return model
    except Exception as e:
        logging.error("â€¼ï¸ æ¨¡å‹æ„å»ºå¤±è´¥", exc_info=True)
        sys.exit(1)

# ================== è®­ç»ƒæµç¨‹ä¼˜åŒ– ==================
def train_model():
    """è®­ç»ƒä¸»æµç¨‹"""
    try:
        # ================== æ•°æ®å‡†å¤‡é˜¶æ®µ ==================
        # åŠ è½½æ•°æ®
        df = load_data()
        data_size = len(df)
        
        # åŠ¨æ€è®¡ç®—æ¨¡å‹å‚æ•°
        model_cfg = calculate_model_capacity(data_size)
        logging.info(f"ğŸ“Š æ¨¡å‹åŠ¨æ€é…ç½® | éšè—å±‚æ•°: {model_cfg['num_layers']} | ç¥ç»å…ƒæ•°: {model_cfg['hidden_units']} | ç¼©æ”¾å› å­: {model_cfg['scaling_factor']:.2f}")

        # æ„å»ºé¢„å¤„ç†å™¨
        preprocessor = build_preprocessor()
        X = df.drop(["height", "weight", "leg_type"], axis=1)
        y_reg = df[["height", "weight"]].values.astype(np.float32)
        y_cls = df["leg_type"].values.astype(np.int32)

        # ================== æ•°æ®åˆ†å‰² ==================
        X_train, X_val, y_reg_train, y_reg_val, y_cls_train, y_cls_val = train_test_split(
            X, y_reg, y_cls, 
            test_size=0.2, 
            stratify=df["leg_type"],
            random_state=42
        )

        # ================== ç‰¹å¾å·¥ç¨‹ ==================
        X_train_processed = preprocessor.fit_transform(X_train)
        X_val_processed = preprocessor.transform(X_val)
        
        # è¾“å…¥ç»´åº¦éªŒè¯
        input_shape_cat = (X_train_processed[:, :5].shape[1],)
        input_shape_num = (X_train_processed[:, 5:].shape[1],)
        if input_shape_cat[0] != 5 or input_shape_num[0] != 8:
            raise ValueError(f"è¾“å…¥ç»´åº¦å¼‚å¸¸ | é¢„æœŸ: åˆ†ç±»5ç»´+æ•°å€¼8ç»´ï¼Œå®é™…: {input_shape_cat}+{input_shape_num}")

        # ================== æ¨¡å‹æ„å»º ==================
        strategy = tf.distribute.MirroredStrategy() if len(tf.config.list_physical_devices('GPU')) > 1 else tf.distribute.get_strategy()
        with strategy.scope():
            model = build_dynamic_model(input_shape_cat, input_shape_num, model_cfg)

        # ================== è®­ç»ƒå‚æ•° ==================
        batch_size = min(256, int(32 * model_cfg["scaling_factor"]))
        epochs = int(50 * (1 + model_cfg["scaling_factor"]))
        logging.info(f"âš™ï¸ è®­ç»ƒå‚æ•° | æ‰¹æ¬¡å¤§å°: {batch_size} | è®­ç»ƒè½®æ¬¡: {epochs}")

        # ================== è®­ç»ƒé…ç½® ==================
        callbacks = [
            tf.keras.callbacks.EarlyStopping(
                patience=max(5, int(5 * model_cfg["scaling_factor"])),
                restore_best_weights=True
            ),
            tf.keras.callbacks.ModelCheckpoint(
                filepath=os.path.join(MODEL_DIR, "footprint_model.h5"),
                monitor="val_classification_accuracy",
                mode="max",
                save_best_only=True,
                save_format="h5"
            ),
            tf.keras.callbacks.CSVLogger(
                os.path.join(MODEL_DIR, "training_log.csv")
            )
        ]

        # ================== å¼€å§‹è®­ç»ƒ ==================
        history = model.fit(
            [X_train_processed[:, :5].astype(np.float32), X_train_processed[:, 5:].astype(np.float32)],
            [y_reg_train, y_cls_train],
            validation_data=(
                [X_val_processed[:, :5].astype(np.float32), X_val_processed[:, 5:].astype(np.float32)],
                [y_reg_val, y_cls_val]
            ),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=2
        )

        # ================== ä¿å­˜ç»“æœ ==================
        # åˆ†åˆ«ä¿å­˜é¢„å¤„ç†å™¨å’Œå…ƒæ•°æ®
        joblib.dump(preprocessor, os.path.join(MODEL_DIR, "preprocessor.pkl"))  # å•ç‹¬ä¿å­˜é¢„å¤„ç†å™¨
        joblib.dump(
            {
                "model_config": model_cfg,
                "training_history": history.history,
                "input_shapes": {
                    "categorical": input_shape_cat,
                    "numerical": input_shape_num
                }
            },
            os.path.join(MODEL_DIR, "training_metadata.pkl")  # å…ƒæ•°æ®å¦å­˜æ–°æ–‡ä»¶
        )

        # æ¨¡å‹é‡åŒ–å‹ç¼©
        converter = tf.lite.TFLiteConverter.from_keras_model(model)
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        tflite_model = converter.convert()
        with open(os.path.join(MODEL_DIR, "footprint_model.tflite"), "wb") as f:
            f.write(tflite_model)

        # ================== è®­ç»ƒæŠ¥å‘Š ==================
        best_epoch = np.argmax(history.history["val_classification_accuracy"])
        logging.info("ğŸ† æœ€ç»ˆè®­ç»ƒæŠ¥å‘Š:")
        logging.info(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {history.history['val_classification_accuracy'][best_epoch]:.2%}")
        logging.info(f"å›å½’ä»»åŠ¡MAE: {history.history['regression_mae'][best_epoch]:.2f}")
        logging.info(f"æ¨¡å‹æ–‡ä»¶: {os.path.join(MODEL_DIR, 'footprint_model.h5')}")
        logging.info(f"é¢„å¤„ç†å™¨æ–‡ä»¶: {os.path.join(MODEL_DIR, 'preprocessor.pkl')}")
        logging.info(f"è®­ç»ƒå…ƒæ•°æ®: {os.path.join(MODEL_DIR, 'training_metadata.pkl')}")

    except Exception as e:
        logging.error("â€¼ï¸ è®­ç»ƒæµç¨‹å¼‚å¸¸ç»ˆæ­¢", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    train_model()
